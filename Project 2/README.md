# Data Modeling with Postgres


## Introduction

Sparkify, a startup, has been collecting data on songs and user activity through their music streaming app. The analytics team at Sparkify wants to gain insights into the songs that users are listening to. However, currently, they lack an efficient way to query their data, which is stored in JSON logs containing user activity and JSON metadata describing the songs in the app.

To address this challenge, a data engineer is assigned to the project. The data engineer's objective is to create a Postgres database and develop an ETL pipeline. This pipeline will facilitate easy access to the data and optimize queries for song play analysis. By implementing these solutions, Sparkify will be able to perform comprehensive analysis and derive meaningful insights from their music streaming data.


## Project Description

In this project, I will be using a star schema to create a fact table and dimension tables. The fact table will contain the primary measures and metrics I want to analyze, while the dimension tables will provide additional context and attributes related to the data.

To populate these tables, I will implement an ETL pipeline. This pipeline will extract the data from the files stored in the /data folder and transform it to fit the structure of the tables. Finally, the transformed data will be loaded into Postgres, allowing us to efficiently query and analyze the information.


## Project Datasets

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

data/song_data/A/B/C/TRABCEI128F424C983.json
data/song_data/A/A/B/TRAABJL12903CDCF1A.json


### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

data/log_data/2018/11/2018-11-12-events.json
data/log_data/2018/11/2018-11-13-events.json


## Database schema

As part of the data modeling process, I have created tables in a star schema structure with a focus on normalization. The tables include:

1. Fact table: `songplays`
   - This table represents the central entity and captures information about the songs played by users.
   - It contains foreign keys referencing the dimension tables and additional details such as timestamps and user activity.

2. Dimension tables:
   a. `users`
      - This table stores information about the users of the music streaming app.
      - It includes attributes such as user ID, name, gender, and subscription level.

   b. `artists`
      - This table contains data about the artists whose songs are played on the app.
      - It includes attributes such as artist ID, name, location, and other relevant details.

   c. `time`
      - This table provides information related to the timestamps of the song plays.
      - It includes attributes such as start time, hour, day, week, month, year, and weekday.

   d. `songs`
      - This table holds data about the songs available on the music streaming app.
      - It includes attributes such as song ID, title, artist ID, duration, and other song-related details.

By organizing the data into a star schema, I can efficiently query and analyze the song play data while maintaining data integrity and reducing data redundancy. The star schema facilitates easy navigation between the fact and dimension tables, enabling meaningful insights and analysis.


## ETL Pipeline

The ETL (Extract, Transform, Load) pipeline is responsible for extracting data from the source files, transforming it into the desired format, and loading it into the database tables. In this project, the ETL pipeline consists of two main functions implemented in `etl.py`:

1. `process_song_file`: This function processes the data from the song files. It extracts relevant information such as song details and artist information, and inserts it into the corresponding tables in the database.

2. `process_log_file`: This function processes the data from the log files, which contain user activity information. It extracts information about user interactions, song plays, and time-related details. The data is transformed and loaded into the appropriate tables in the database.


## Run The Scripts

To run the ETL pipeline in a Jupyter notebook, follow these steps:

1. Execute the `create_tables.py` script to generate the necessary tables in the database. This ensures that the database is properly set up to receive the data.

   ```
   %run create_tables.py
   ```

2. Run the `etl.py` script to perform the ETL process. This script will process the song files and log files, transforming the data and inserting it into the corresponding tables in the database.

   ```
   %run etl.py
   ```


## Conclusion
In conclusion, the implementation of the data modeling and ETL pipeline for Sparkify's music streaming app has provided a well-structured database that enables efficient querying and analysis of user activity and song data, empowering the analytics team to derive valuable insights for business decision-making.
